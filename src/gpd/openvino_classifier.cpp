/*
 * Software License Agreement (BSD License)
 *
 *  Copyright (c) 2018 Intel Corporation
 *  All rights reserved.
 *
 *  Redistribution and use in source and binary forms, with or without
 *  modification, are permitted provided that the following conditions
 *  are met:
 *
 *   * Redistributions of source code must retain the above copyright
 *     notice, this list of conditions and the following disclaimer.
 *   * Redistributions in binary form must reproduce the above
 *     copyright notice, this list of conditions and the following
 *     disclaimer in the documentation and/or other materials provided
 *     with the distribution.
 *
 *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 *  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 *  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
 *  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
 *  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 *  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 *  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 *  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
 *  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 *  POSSIBILITY OF SUCH DAMAGE.
 */

#include "gpd/openvino_classifier.h"

using namespace InferenceEngine;

std::map<Classifier::Device, InferenceEngine::TargetDevice> OpenVINOClassifier::device_map_ = {
  {Classifier::Device::eCPU, TargetDevice::eCPU},
  {Classifier::Device::eGPU, TargetDevice::eGPU},
  {Classifier::Device::eVPU, TargetDevice::eMYRIAD},
  {Classifier::Device::eFPGA, TargetDevice::eFPGA}};

OpenVINOClassifier::OpenVINOClassifier(Classifier::Device device)
{
  // ---------------------Load MKLDNN Plugin for Inference Engine-------------------------------------------------------
  InferenceEngine::PluginDispatcher dispatcher({"../../../lib/intel64", ""});
  plugin_ = InferencePlugin(dispatcher.getSuitablePlugin(device_map_[device]));
  // --------------------Load IR Generated by ModelOptimizer (.xml and .bin files)--------------------------------------
  CNNNetReader network_reader;
  switch (device)
  {
  case Classifier::Device::eCPU:
    network_reader.ReadNetwork(std::string(MODELS_DIR) + "/fp32/single_view_15_channels.xml");
    network_reader.ReadWeights(std::string(MODELS_DIR) + "/fp32/single_view_15_channels.bin");
    break;
  case Classifier::Device::eVPU:
    network_reader.ReadNetwork(std::string(MODELS_DIR) + "/fp16/single_view_15_channels.xml");
    network_reader.ReadWeights(std::string(MODELS_DIR) + "/fp16/single_view_15_channels.bin");
    break;
  case Classifier::Device::eGPU:
    std::cout << "GPU device to be supported!!\n";
    // fall through;
  case Classifier::Device::eFPGA:
    std::cout << "FPGA device to be supported!!\n";
    // fall through;
  default:
    throw std::exception();
  }
  network_ = network_reader.getNetwork();
  network_.setBatchSize(1);

  // -----------------------------Prepare input blobs-------------------------------------------------------------------
  auto input_info = network_.getInputsInfo().begin()->second;
  auto input_name = network_.getInputsInfo().begin()->first;
  input_info->setPrecision(Precision::FP32);

  // ---------------------------Prepare output blobs--------------------------------------------------------------------
  auto output_info = network_.getOutputsInfo().begin()->second;
  auto output_name = network_.getOutputsInfo().begin()->first;
  output_info->setPrecision(Precision::FP32);

  // -------------------------Loading model to the plugin---------------------------------------------------------------
  std::cout << "network output: "<< output_name << ", input: " <<  input_name << "\n";
  infer_request_ = plugin_.LoadNetwork(network_, {}).CreateInferRequest();
  input_blob_ = infer_request_.GetBlob(input_name);
  output_blob_ = infer_request_.GetBlob(output_name);

  // -------------------------Preparing the input blob buffer-----------------------------------------------------------
  auto input_data = input_blob_->buffer().as<PrecisionTrait<Precision::FP32>::value_type*>();
  batch_image_list_.clear();
  for (int i = 0; i < input_blob_->dims()[2]; ++i) {
    cv::Mat img(input_blob_->dims()[0], input_blob_->dims()[1], CV_32FC1, input_data);
    batch_image_list_.push_back(img);
    input_data += input_blob_->dims()[0] * input_blob_->dims()[1];
  }
}

std::vector<float> OpenVINOClassifier::classifyImages(const std::vector<cv::Mat>& image_list)
{
  std::vector<float> predictions;
  std::cout << "# images: " << image_list.size() << "\n";

  for (int i = 0; i < image_list.size(); i++)
  {
    cv::Mat img;
    image_list[i].convertTo(img, CV_32FC(input_blob_->dims()[2]));
    cv::split(img, batch_image_list_);

    // ---------------------------Running the inference request synchronously-------------------------------------------
    infer_request_.Infer();

    // ---------------------------Postprocess output blobs--------------------------------------------------------------
    auto output_data = output_blob_->buffer().as<PrecisionTrait<Precision::FP32>::value_type*>();
    // std::cout << "positive score: " << output_data[1] << ", negative score: " << output_data[0] << "\n";
    predictions.push_back(output_data[1] - output_data[0]);
  }

  return predictions;
}

int OpenVINOClassifier::getBatchSize() const
{
  return network_.getBatchSize();
}
